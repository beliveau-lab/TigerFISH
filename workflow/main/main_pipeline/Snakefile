import os
from os import path
import glob

configfile: "config.yml"

SAMPLES = config["samples"]
ASSEMBLY = config["assembly"]
BOWTIE2_DIR = config["bowtie2_dir"]
JF_HASH = config['jf_hash_dir']
JF_COUNT = config['jf_count_dir']
CHROM_IDX = config['chrom_idx_dir']
CHROM_FASTA = config['chrom_fasta_dir']

rule all:
    input:
        expand("pipeline_output/finished/DONE_{sample}.txt",sample=SAMPLES)

if (config['defined_coords'] == "TRUE" and config['repeat_discovery'] == "TRUE") or (config['defined_coords'] == "FALSE" and config['repeat_discovery'] == "FALSE"):
    print("Repeat_Discovery Mode and Probe_Design Mode can only be run independently. Please select TRUE for desired workflow and FALSE for the other process. Exiting ...")
    exit()

if config['jf_hash_given'] == "FALSE":
    rule generate_jf_count:
        input:
            fasta_file = config["fasta_file"]
        conda:
            "../../../shared_conda_envs/tigerfish.yml"
        params:
            mer = config["mer_val"],
            mfree="60G",
            h_rt = "20:0:0"
        benchmark:
            'pipeline_output/benchmarks/01_reference_files/01_jf_query_file/jf_count_log.log'
        output:
            'pipeline_output/01_reference_files/01_jf_query_file/genome_query.jf'
        shell:
            "jellyfish count -s 3300M -m {params.mer} -o {output} -C {input.fasta_file}"

def input_for_bowtie(wildcards):
    # requires a config containing switches for the whole workflow
    if config["bowtie2_indices_given"]=="TRUE":
        return BOWTIE2_DIR
    elif config["bowtie2_indices_given"]=="FALSE":
        return rules.generate_bt2_indices.output

def input_for_jf_idx(wildcards):
    #requires a config containing switches for the workflow
    if config['jf_hash_given'] == "TRUE":
        return JF_HASH
    elif config['jf_hash_given'] == "FALSE":
        return rules.generate_jf_count.output

if config['bowtie2_indices_given'] == "FALSE":

    BOWTIE2_DIR = f'pipeline_output/01_reference_files/02_bt2_idx/{ASSEMBLY}'
    rule generate_bt2_indices:
        input:
            fasta_file = config["fasta_file"]
        conda:
            "../../../shared_conda_envs/tigerfish.yml"
        params:
            mfree="60G",
            h_rt = "20:0:0"
        benchmark:
            'pipeline_output/benchmarks/01_reference_files/02_bt2_idx/bt2_idx_log.log'
        output:
            f'{BOWTIE2_DIR}/{ASSEMBLY}.1.bt2',
            f'{BOWTIE2_DIR}/{ASSEMBLY}.2.bt2',
            f'{BOWTIE2_DIR}/{ASSEMBLY}.3.bt2',
            f'{BOWTIE2_DIR}/{ASSEMBLY}.4.bt2',
            f'{BOWTIE2_DIR}/{ASSEMBLY}.rev.1.bt2',
            f'{BOWTIE2_DIR}/{ASSEMBLY}.rev.2.bt2',
        shell:
            'bowtie2-build --threads 4 {input} {BOWTIE2_DIR}/{ASSEMBLY}'

def input_for_jf_count_file(wildcards):
    #requires config containing switches for the workflow
    if config['jf_count_given'] == "TRUE":
        return JF_COUNT
    elif config['jf_count_given'] == "FALSE":
        return rules.generate_jf_idx.output.jf_count

def input_for_chrom_idx_file(wildcards):
    #requires config containing switches for the workflow
    if config['chrom_idx_given']  == "TRUE":
        return CHROM_IDX
    elif config['chrom_idx_given'] == "FALSE":
        return rules.generate_jf_idx.output.chrom_idx

def input_for_chrom_fasta_file(wildcards):
    #requires config containing switches for the workflow
    if config['chrom_fasta_given']  == "TRUE":
        return CHROM_FASTA
    elif config['chrom_fasta_given'] == "FALSE":
        return rules.generate_jf_idx.output.chrom_fa

if config['jf_hash_given'] == 'FALSE':
    rule generate_jf_idx:
        input:
            fasta_file = config["fasta_file"],
            jf = input_for_jf_idx
        conda:
            "../../../shared_conda_envs/tigerfish.yml"
        params:
            chrom_name = "{sample}",
            mer = config["mer_val"],
            mfree="60G",
            h_rt = "20:0:0"
        benchmark:
            'pipeline_output/benchmarks/01_reference_files/03_jf_idx/{sample}_log.log'
        output:
            jf_count = 'pipeline_output/01_reference_files/03_jf_idx/{sample}_jf_temp.txt',
            chrom_idx = 'pipeline_output/01_reference_files/03_jf_idx/{sample}_index.txt',
            chrom_fa = 'pipeline_output/01_reference_files/03_jf_idx/repeat_fasta/{sample}.fa'
        shell:
            "python ../../../../workflow/main/scripts/generate_jf_idx.py -f {input.fasta_file} -j {input.jf} -c {params.chrom_name} -m {params.mer} -f_o {output.chrom_fa} -j_o {output.jf_count} -i {output.chrom_idx}"

if config['defined_coords'] == "TRUE":
    rule split_bed:
        input:
            bed_file = config["bed_file"]
        conda:
            "../../../shared_conda_envs/tigerfish.yml"
        params:
            chrom_name = "{sample}",
            mfree="10G",
            h_rt = "3:0:0"
        benchmark:
            'pipeline_output/benchmarks/02_intermediate_files/01_split_bed/{sample}_log.log'
        output:
            chrom_region = 'pipeline_output/02_intermediate_files/01_split_bed/{sample}_regions.bed'
        shell:
            "python ../../../../workflow/main/scripts/split_bed.py -b {input.bed_file} -c {params.chrom_name} -o {output.chrom_region}"

if config['repeat_discovery'] == "TRUE":
    rule repeat_ID:
        input:
            jf_count = input_for_jf_count_file,
            chrom_index = input_for_chrom_idx_file
        conda:
            "../../../shared_conda_envs/tigerfish.yml"
        params:
            window = config["window"],
            threshold = config["threshold"],
            composition = config["composition"],
            file_start = config["file_start"],
            chrom_name = "{sample}",
            mer = config["mer_val"],
            mfree="85G",
            h_rt="200:0:0"
        benchmark:
            "pipeline_output/benchmarks/02_intermediate_files/01_repeat_ID/{sample}_log.log"
        output:
            out_bed = "pipeline_output/02_intermediate_files/01_repeat_ID/{sample}_regions.bed",
        shell:
            "python ../../../../workflow/main/scripts/repeat_ID.py -j {input.jf_count} -i {input.chrom_index} -m {params.mer} -w {params.window} -t {params.threshold} -c {params.composition} -chr {params.chrom_name} -st {params.file_start} -o_b {output.out_bed}"

def input_for_design_probes(wildcards):
    # requires a config containing switches for the whole workflow
    if config["defined_coords"]=='TRUE':
        return 'pipeline_output/02_intermediate_files/01_split_bed/{sample}_regions.bed'
    elif config["repeat_discovery"]=='TRUE':
        return 'pipeline_output/02_intermediate_files/01_repeat_ID/{sample}_regions.bed'

rule design_probes:
    input:
        region_bed = input_for_design_probes,
        chr_path = input_for_chrom_fasta_file
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="60G",
        h_rt="200:0:0",
        chrom_name = "{sample}",
        min_length = config["min_length"],
        max_length = config["max_length"],
        min_temp = config["min_temp"],
        max_temp = config["max_temp"]
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/02_designed_probes/{sample}_log.log"
    output:
        designed_probes = "pipeline_output/02_intermediate_files/02_designed_probes/{sample}_blockParse_probe_df.bed",
        probe_fa = "pipeline_output/02_intermediate_files/02_designed_probes/{sample}_probe_regions.fa"
    shell:
        "python ../../../../workflow/main/scripts/design_probes.py -b {input.region_bed} -c {params.chrom_name} -g {input.chr_path} -p_o {output.designed_probes} -r_o {output.probe_fa} -l {params.min_length} -L {params.max_length} -t {params.min_temp} -T {params.max_temp}"

rule kmer_filter:
    input:
        jf = input_for_jf_count_file,
        probes = rules.design_probes.output.designed_probes,
        region_fa = rules.design_probes.output.probe_fa
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="50G",
        h_rt="200:0:0",
        mer = config["mer_val"],
        chrom_name = "{sample}",
        c1 = config["c1_val"],
        c2 = config["c2_val"]
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/03_pre_filter/{sample}_log.log"
    output:
        "pipeline_output/02_intermediate_files/03_pre_filter/{sample}_probes_pre_filter.txt"
    shell:
        "python ../../../../workflow/main/scripts/kmer_filter.py -p {input.probes} -o {output} -j {input.jf} -f {input.region_fa} -m {params.mer} -c1 {params.c1} -c2 {params.c2} -c {params.chrom_name}"

rule probe_mer_filter:
    input:
        probes = rules.kmer_filter.output
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="50G",
        h_rt="200:0:0",
        mer = config["mer_val"],
        enrich = config["enrich_score"],
        copy_num = config["copy_num"],
        mer_cutoff = config["mer_cutoff"]
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/04_post_mer_filter/{sample}_log.log"
    output:
        "pipeline_output/02_intermediate_files/04_post_mer_filter/{sample}_probes_mer_filter.txt"
    shell:
        "python ../../../../workflow/main/scripts/probe_mer_filter.py -f {input.probes} -o {output} -e {params.enrich} -cn {params.copy_num} -m {params.mer_cutoff} -k {params.mer}"

rule generate_genome_bins:
    input:
        sizes = config["chrom_sizes_file"]
    output:
        alignment_bins = "pipeline_output/01_reference_files/01_bin_genome/genome_bins_tigerfish_alignment.bed",
        threshold_bins = "pipeline_output/01_reference_files/01_bin_genome/genome_bins_tigerfish_threshold.bed"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="20G",
        h_rt = "10:0:0",
        genome = config['assembly'],
        window = config['genome_windows'],
        thresh_window = config['thresh_window']
    benchmark:
        "pipeline_output/benchmarks/01_reference_files/04_bin_genome/genome_bins.txt"
    shell:
        """
        bedtools makewindows -g {input.sizes} -w {params.window} > {output.alignment_bins}
        bedtools makewindows -g {input.sizes} -w {params.thresh_window} > {output.threshold_bins}
        """

checkpoint make_chrom_dir:
    input:
       probes = rules.probe_mer_filter.output
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="5G",
        h_rt="350:0:0"
    output:
        region_split = directory("pipeline_output/02_intermediate_files/05_split_regions/{sample}/")
    shell:
        """
        python ../../../../workflow/main/scripts/split_filter.py -f {input.probes} -o {output}
        """

rule alignment_filter:
    input:
        probe_files = "pipeline_output/02_intermediate_files/05_split_regions/{sample}/{region}.txt",
        genome_bins = rules.generate_genome_bins.output.alignment_bins,
        BOWTIE2_DIR = input_for_bowtie
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="25G",
        h_rt="350:0:0",
        region_thresh = config['target_sum'], 
        k_val = config['bt2_alignments'],
        max_pdups_binding = config['max_pdups_binding'],
        seed_length = config["seed_length"],
        model_temp = config["model_temp"],
        min_on_target = config["min_on_target"],
        max_probe_return = config["max_probe_return"],
        off_bin_thresh = config["off_bin_thresh"],
        binding_prop = config['binding_prop'],
        ref_flag = config['ref_flag']
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/06_alignment/{sample}/{region}.log"
    output:
        "pipeline_output/02_intermediate_files/06_alignment/{sample}/{region}_alignment.txt"
    shell:
        "python ../../../../workflow/main/scripts/alignment_filter.py -f {input.probe_files} -b {BOWTIE2_DIR}/{ASSEMBLY} -o {output} -r {params.region_thresh} -p {params.binding_prop} -k {params.k_val} -pb {params.max_pdups_binding} -l {params.seed_length} -t {params.model_temp} -moT {params.min_on_target} -Mr {params.max_probe_return} -gb {input.genome_bins} -th {params.off_bin_thresh} -rf {params.ref_flag}"

# aggregate paths to dynamically created files
def aggregate_alignment_input(wildcards):
    checkpoint_output = checkpoints.make_chrom_dir.get(**wildcards).output[0]
    return expand("pipeline_output/02_intermediate_files/06_alignment/{sample}/{region}_alignment.txt",
           sample=wildcards.sample,
           region=glob_wildcards(os.path.join(checkpoint_output, "{region}.txt")).region)

#aggregates alignment files
rule merge_alignment_filter:
    input:
        aggregate_alignment_input
    output:
        'pipeline_output/02_intermediate_files/07_merged_alignments/{sample}_alignments.tsv'
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree='3G',
        h_rt='3:0:0'
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/07_merged_alignments/{sample}.log"
    shell:
        "cat {input} > {output}"

#splits all regions remaining into seperate files in new directory
checkpoint split_rm_alignments:
    input:
        rules.merge_alignment_filter.output
    output:
        region_split = directory("pipeline_output/02_intermediate_files/08_split_align/{sample}/")
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree='3G',
        h_rt='3:0:0'
    benchmark:
        "pipeline_output/02_intermediate_files/08_split_align/{sample}.log"
    shell:
        "python ../../../../workflow/main/scripts/split_rm_alignments.py -f {input} -o {output}"

#proceeds with independent region probe alignment
rule align_probes:
    input:
        "pipeline_output/02_intermediate_files/08_split_align/{sample}/{region}_alignments.txt"
    output:
        "pipeline_output/02_intermediate_files/09_probe_alignment/{sample}/{region}_probe_alignment.txt"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="20G",
        h_rt = "10:0:0",
        k_val = config["bt2_alignments"],
        seed_length = config["seed_length"],
        model_temp = config["model_temp"]
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/09_probe_alignment/{sample}/{region}_alignment.txt"
    shell:
        'python ../../../../workflow/main/scripts/generate_alignments.py -f {input} -o {output} -b {BOWTIE2_DIR}/{ASSEMBLY} -k {params.k_val} -l {params.seed_length} -t {params.model_temp}'

#rule to make probe alignments into bed regions
rule derived_beds:
    input:
        rules.align_probes.output
    output:
        "pipeline_output/02_intermediate_files/10_derived_beds/{sample}/{region}_derived.bed"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="20G",
        h_rt = "10:0:0"
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/10_derived_beds/{sample}/{region}_derived.txt"
    shell:
        "python ../../../../workflow/main/scripts/make_derived_beds.py -f {input} -o {output}"

rule get_region_bed:
    input:
        rules.alignment_filter.output
    output:
        "pipeline_output/02_intermediate_files/11_get_repeat_bed/{sample}/{region}.bed"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="20G",
        h_rt = "10:0:0"
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/11_get_repeat_bed/{sample}/{region}.txt"
    shell:
        "python ../../../../workflow/main/scripts/get_region_bed.py -i {input} -o {output}"

rule bedtools_intersect:
    input:
        derived_bed = rules.derived_beds.output,
        genome_bin = rules.generate_genome_bins.output.threshold_bins,
        repeat_bed = rules.get_region_bed.output
    output:
        alignments_out = "pipeline_output/02_intermediate_files/12_bedtools_intersect/{sample}/{region}_intersect.txt",
        repeat_out = "pipeline_output/02_intermediate_files/12_bedtools_intersect/{sample}/{region}_repeat_intersect.txt"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="20G",
        h_rt = "320:0:0"
    benchmark:
        "pipeline_output/benchmarks/02_intermediate_files/12_bedtools_intersect/{sample}/{region}_derived.txt"
    shell:
        "bedtools intersect -wa -wb -a {input.derived_bed} -b {input.genome_bin} > {output.alignments_out} |"
        "bedtools intersect -wa -wb -a {input.repeat_bed} -b {input.genome_bin} > {output.repeat_out}"

#rule to accept bins, alignments and map target/off-target (checks if any bin > 100) to cull probe
rule get_alignments:
    input:
        alignment_intersect = rules.bedtools_intersect.output.alignments_out,
        region_intersect = rules.bedtools_intersect.output.repeat_out,
        probes_alignment = rules.align_probes.output,
        genome_bin = rules.generate_genome_bins.output.threshold_bins
    output:
        target_binding = "pipeline_output/03_output_files/02_generate_probe_binding/01_genome_wide/{sample}/{region}_alignment_binding.tsv",
        thresh_binding = "pipeline_output/03_output_files/02_generate_probe_binding/02_threshold/{sample}/{region}_thresh_binding.tsv",
        binding_maps = "pipeline_output/03_output_files/02_generate_probe_binding/03_plots/{sample}/{region}_genome_view.png"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    benchmark:
        "pipeline_output/benchmarks/03_output_files/02_get_alignments/{sample}/{region}_alignments.txt"
    params:
        mfree="20G",
        h_rt = "320:0:0",
        thresh = config["align_thresh"]    
    shell:
        "python ../../../../workflow/main/scripts/get_alignments.py -c_t {input.genome_bin} -c_o {input.alignment_intersect} -p {input.probes_alignment} -r_o {input.region_intersect} -pl {output.binding_maps} -t {params.thresh} -t_s {output.thresh_binding} -c_s {output.target_binding}"

rule generate_chromomap:
    input:
        chrom_sizes = config["chrom_sizes_file"],
        region_bed = rules.get_region_bed.output,
        binding_maps = rules.get_alignments.output.binding_maps
    output:
        "pipeline_output/03_output_files/06_chromomap/{sample}/{region}_chromomap.html"
    conda:
        "../../../shared_conda_envs/chromomap_env.yml"
    params:
        mfree="20G",
        h_rt = "10:0:0"
    benchmark:
        "pipeline_output/benchmarks/03_pipeline_output/06_chromomap/{sample}/{region}_plots.txt"
    shell:
        "Rscript --vanilla ../../../../workflow/main/scripts/make_chromomap.R -c {input.chrom_sizes} -r {input.region_bed} -o {output}"

# aggregate paths to dynamically created files
def all_mapped_output(wildcards):
    checkpoint_output = checkpoints.split_rm_alignments.get(**wildcards).output[0]
    return expand("pipeline_output/03_output_files/03_probes/{sample}/{region}_probes.tsv",
           sample=wildcards.sample,
           region=glob_wildcards(os.path.join(checkpoint_output, "{region}_alignments.txt")).region)

rule map_region_coords:
    input:
        probe_file = rules.alignment_filter.output,
        thresh_file = rules.get_alignments.output.thresh_binding,
        alignment_file = rules.align_probes.output,
        maps = rules.generate_chromomap.output
    output:
        mod_probe_file = 'pipeline_output/03_output_files/03_probes/{sample}/{region}_probes.tsv',
        repeat_binding_summ = 'pipeline_output/03_output_files/04_repeat_output/{sample}/{region}_thresh_summ.tsv'
    params:
        mfree="20G",
        h_rt = "10:0:0"
    benchmark:
        "pipeline_output/benchmarks/03_pipeline_output/03_map_region/{sample}/{region}.txt"
    shell:
        "python ../../../../workflow/main/scripts/collapse_repeat.py -f {input.thresh_file} -a {input.alignment_file} -p {input.probe_file} -po {output.mod_probe_file} -ro {output.repeat_binding_summ}"

# merge the individually-flattened chromosome annotation files
rule merge_mapping:
    input:
        all_mapped_output
    output:
        'pipeline_output/03_output_files/01_all_probes/{sample}_all_probes.tsv'
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree='3G',
        h_rt='3:0:0'
    shell:
        "cat {input} > {output}"

rule summary:
    input:
        rules.merge_mapping.output
    output:
        "pipeline_output/03_output_files/05_probe_summary/{sample}_probe_summary.txt"
    conda:
        "../../../shared_conda_envs/tigerfish.yml"
    params:
        mfree="3G",
        h_rt="200:0:0"
    benchmark:
        "pipeline_output/benchmarks/03_output_files/04_probe_summary/{sample}_probe_summary.txt"
    shell:
        "python ../../../../workflow/main/scripts/finish_summary.py -f {input} -o {output}"

rule finish:
    input:
        rules.summary.output
    output:
        'pipeline_output/finished/DONE_{sample}.txt'
    params:
        mfree = '3G',
        h_rt = '3:0:0'
    shell:
        'touch {output}'
